{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/sk-classroom/asc-bert/blob/main/assignments/assignment_01.ipynb)\n",
    "\n",
    "We will learn how to generate word embeddings using BERT. BERT produces contextualized word embeddings, where the embeddings are computed based on the context of the word. Thus, a single word can have different embeddings based on its context. \n",
    "\n",
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed the required packages, please install them using pip\n",
    "# pip install transformers plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skojaku/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n",
    "\n",
    "We will use [CoarseWSD-20](https://github.com/danlou/bert-disambiguation/tree/master/data/CoarseWSD-20). The dataset contains sentences with polysemous words and their sense labels. We will see how to use BERT to disambiguate the word senses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>there also exist javascript and java backends ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>it is found on java .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>typed object-oriented programming languages , ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>flying saucer ( also called xhtml renderer ) i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>november 2013 jelastic announced a partnership...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>they also traveled to bali , borneo , java , c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>sunan bayat is often mentioned in the javanese...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18</td>\n",
       "      <td>mucommander is a lighweight , open-source , cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>following graduation , matthews spent six year...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26</td>\n",
       "      <td>time zones many computer operating systems ( s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_pos                                           sentence  label\n",
       "0         5  there also exist javascript and java backends ...      1\n",
       "1         4                              it is found on java .      0\n",
       "2         7  typed object-oriented programming languages , ...      1\n",
       "3        11  flying saucer ( also called xhtml renderer ) i...      1\n",
       "4        82  november 2013 jelastic announced a partnership...      1\n",
       "5         8  they also traveled to bali , borneo , java , c...      0\n",
       "6        21  sunan bayat is often mentioned in the javanese...      0\n",
       "7        18  mucommander is a lighweight , open-source , cr...      1\n",
       "8         8  following graduation , matthews spent six year...      0\n",
       "9        26  time zones many computer operating systems ( s...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(focal_word, is_train):\n",
    "    data_type = \"train\" if is_train else \"test\"\n",
    "    data_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.data.txt\"\n",
    "    label_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.gold.txt\"\n",
    "\n",
    "    data_table = pd.read_csv(\n",
    "        data_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"word_pos\": int, \"sentence\": str},\n",
    "        names=[\"word_pos\", \"sentence\"],\n",
    "    )\n",
    "    label_table = pd.read_csv(\n",
    "        label_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"label\": int},\n",
    "        names=[\"label\"],\n",
    "    )\n",
    "    combined_table = pd.concat([data_table, label_table], axis=1)\n",
    "    return combined_table\n",
    "\n",
    "\n",
    "focal_word = \"java\"\n",
    "\n",
    "train_data = load_data(focal_word, is_train=False)\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the [README](https://github.com/danlou/bert-disambiguation/blob/master/data/CoarseWSD-20/README.txt) for the data. \n",
    "\n",
    "## Define BERT model\n",
    "\n",
    "We will use `transformers` library developed by Hugging Face to define the BERT model. To use the model, we will need:  \n",
    "1. BERT tokenizer that converts the text into tokens. \n",
    "2. BERT model that computes the embeddings of the tokens. \n",
    "\n",
    "We will use the `bert-base-uncased` model and tokenizer. Let's define the model and tokenizer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.BertModel.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With BERT, we need to prepare text in ways that BERT can understand. \n",
    "Specifically, we prepend it with ```[CLS]``` and append ```[SEP]```. We will then convert the text to a tensor of token ids, which is ready to be fed into the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = torch.ones((1, len(indexed_tokens)), dtype=torch.long)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = segments_ids.clone()\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is sentence tensor?\n",
    "BERT models are designed to process sentence pairs, differentiated by 0s and 1s to indicate the first and second sentence respectively. In the case of single-sentence inputs, we assign a vector of 1s to each token, indicating they all belong to the first sentence.\n",
    "\n",
    "Let's get the BERT embeddings for the sentence \"Bank is located in the city of London\". \n",
    "\n",
    "First, let's prepare the text for BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bank is located in the city of London\"\n",
    "tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's get the BERT embeddings for each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(tokens_tensor, segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output includes `loss`, `logits`, and `hidden_states`. We will use `hidden_states`, which contains the embeddings of the tokens. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many layers?  13\n",
      "Shape?  torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "print(\"how many layers? \", len(hidden_states))\n",
    "print(\"Shape? \", hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden states are a list of 13 tensors, where each tensor is of shape (batch_size, sequence_length, hidden_size). The first tensor is the input embeddings, and the subsequent tensors are the hidden states of the BERT layers. \n",
    "\n",
    "So, we have 13 choice of hidden states. Deep layers close to the output capture the context of the word from the previous layers.\n",
    "\n",
    "Here we will take the average over the last four hidden states for each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = torch.cat([hidden_states[-i] for i in range(4)], dim=0).mean(dim=0)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emb is of shape (sequence_length, hidden_size). Let us summarize the embeddings of the tokens into a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    emb = torch.cat([hidden_states[-i] for i in range(4)], dim=0).mean(dim=0)\n",
    "    return emb, tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "Let's embed the text and get the embedding of the focal word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1929/1929 [00:43<00:00, 44.42it/s]\n"
     ]
    }
   ],
   "source": [
    "label_list = []\n",
    "word_emb_list = []\n",
    "sent_list = []\n",
    "for word_pos, sentence, label in tqdm(\n",
    "    train_data[[\"word_pos\", \"sentence\", \"label\"]].itertuples(index=False),\n",
    "    total=train_data.shape[0],\n",
    "):\n",
    "    emb, tokenized_text = get_bert_embeddings(sentence)\n",
    "    word_pos += 1  # BERT tokenizer adds [CLS] and [SEP]\n",
    "\n",
    "    # If the word is not the focal word, continue\n",
    "    if tokenized_text[word_pos] != focal_word:\n",
    "        continue\n",
    "\n",
    "    word_emb_list.append(emb[word_pos].detach().numpy())\n",
    "    label_list.append(label)\n",
    "    sent_list.append(sentence)\n",
    "\n",
    "word_emb_list = np.vstack(word_emb_list)\n",
    "label_list = np.array(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "\n",
    "Let's plot the embeddings of the focal word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'px' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m xy \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(word_emb_list)\n\u001b[0;32m----> 3\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mpx\u001b[49m\u001b[38;5;241m.\u001b[39mscatter(\n\u001b[1;32m      4\u001b[0m     x\u001b[38;5;241m=\u001b[39mxy[:, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      5\u001b[0m     y\u001b[38;5;241m=\u001b[39mxy[:, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      6\u001b[0m     color\u001b[38;5;241m=\u001b[39mlabel_list,\n\u001b[1;32m      7\u001b[0m     hover_data\u001b[38;5;241m=\u001b[39m[sent_list],\n\u001b[1;32m      8\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA of Word Embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m700\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m     11\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'px' is not defined"
     ]
    }
   ],
   "source": [
    "xy = PCA(n_components=2).fit_transform(word_emb_list)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    "    color=label_list,\n",
    "    hover_data=[sent_list],\n",
    "    title=\"PCA of Word Embeddings\",\n",
    ")\n",
    "fig.update_layout(width=700, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "- Run the code for any word available in [the dataset](https://github.com/danlou/bert-disambiguation/tree/master/data/CoarseWSD-20) **except** word \"apple\". Use test data to save time. And save the embeddings of the test data and their labels by running the following cell.  \n",
    "- Make sure to place the generated file \"eval_test.csv\" in the \"data\" folder. \n",
    "- Commit the file to your assignment repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "K = len(set(label_list))\n",
    "\n",
    "xy = LinearDiscriminantAnalysis(n_components=K - 1).fit_transform(\n",
    "    word_emb_list, label_list\n",
    ")\n",
    "\n",
    "xy_df = pd.DataFrame(xy)\n",
    "xy_df[\"label\"] = label_list\n",
    "xy_df.to_csv(\"../data/eval_test.csv\", index=False)\n",
    "\n",
    "# pd.DataFrame({\"x\": xy[:, 0], \"y\": xy[:, 1], \"label\": label_list}).to_csv(\n",
    "#    \"../data/eval_test.csv\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applsoftcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
